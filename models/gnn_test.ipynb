{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from torch_geometric.loader import DataLoader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:11:51.291412Z",
     "start_time": "2024-12-24T23:11:51.289655Z"
    }
   },
   "cell_type": "code",
   "source": "os.chdir('..')",
   "id": "4ef6a39a262ec11b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Преобразование графа в нужный формат",
   "id": "ec96689276d4bc96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:11:51.326029Z",
     "start_time": "2024-12-24T23:11:51.323494Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def nx_to_pyg_data(G, node_characteristics, label):\n",
    "    node_mapping = {node: i for i, node in enumerate(G.nodes())}\n",
    "    \n",
    "    # Преобразуем рёбра с учётом отображения узлов\n",
    "    edge_index = torch.tensor(\n",
    "        [(node_mapping[src], node_mapping[dst]) for src, dst in G.edges()],\n",
    "        dtype=torch.long\n",
    "    ).t().contiguous()\n",
    "    \n",
    "    # Преобразуем признаки узлов в тензор\n",
    "    x = torch.tensor(node_characteristics, dtype=torch.float)\n",
    "    \n",
    "    # Преобразуем метку в тензор\n",
    "    y = torch.tensor([label], dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, y=y)"
   ],
   "id": "6e19031053f3f8c3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Графы для обучения",
   "id": "4fa7c1dcc54afe24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:11:51.331697Z",
     "start_time": "2024-12-24T23:11:51.329864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "small_components_all = {}\n",
    "small_components_families_all = {}\n",
    "largest_component_all = {}\n",
    "largest_component_families_all = {}\n",
    "G_all = {}\n",
    "node2name_dict_all = {}\n",
    "name2seq_all = {}\n",
    "node_freq_dict_all = {}\n",
    "node_family_dict_all = {}"
   ],
   "id": "4664b7abc9e53e3d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:11:51.341996Z",
     "start_time": "2024-12-24T23:11:51.337868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Функция для загрузки данных из файла\n",
    "def load_data_from_file(filename):\n",
    "    try:\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_components(name):\n",
    "    small_components_filename = os.path.join('data', f'data_{name}.pkl')\n",
    "    if os.path.exists(small_components_filename):\n",
    "        data = load_data_from_file(small_components_filename)\n",
    "        if data:\n",
    "            G_all[name] = data['graph']\n",
    "            node_freq_dict_all[name] = data['node_freq_dict']\n",
    "            node_family_dict_all[name] = data['node_family_dict']\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Процесс получения меток для маленьких компонент\n",
    "def process_small_components(name):\n",
    "    small_components_families_filename = os.path.join('data', f'small_components_families_{name}.pkl')\n",
    "    if os.path.exists(small_components_families_filename):\n",
    "        data = load_data_from_file(small_components_families_filename)\n",
    "        if data:\n",
    "            small_components_families_all[name] = data['families']\n",
    "            small_components_all[name] = data['components']\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Процесс получения кластеров и меток для большой компоненты\n",
    "def process_largest_component(name):\n",
    "    largest_component_filename = os.path.join('data', f'largest_component_clusters_{name}.pkl')\n",
    "    if os.path.exists(largest_component_filename):\n",
    "        data = load_data_from_file(largest_component_filename)\n",
    "        if data:\n",
    "            largest_component = data['clusters']\n",
    "            largest_component_families = data['families']\n",
    "            \n",
    "            largest_component_all[name] = largest_component\n",
    "            largest_component_families_all[name] = largest_component_families\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def process_dicts(name):\n",
    "    dicts_filename = os.path.join('data', f'dicts_{name}.pkl')\n",
    "    if os.path.exists(dicts_filename):\n",
    "        data = load_data_from_file(dicts_filename)\n",
    "        if data:\n",
    "            node2name_dict_all[name] = data['node2name_dict']\n",
    "            name2seq_all[name] = data['name2seq']\n",
    "\n",
    "        return True\n",
    "    return False\n",
    "    "
   ],
   "id": "4cb1a5534c53fd80",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:11:51.770279Z",
     "start_time": "2024-12-24T23:11:51.351647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "names = ['juncea', 'nigra', 'rapa']\n",
    "for name in names:\n",
    "    if not process_small_components(name):\n",
    "        print(f\"File for {name} (small components families) not found.\")\n",
    "    \n",
    "    if not process_largest_component(name):\n",
    "        print(f\"File for {name} (largest component) not found.\")\n",
    "        \n",
    "    if not process_components(name):\n",
    "        print(f\"File for {name} (all components) not found.\")\n",
    "    \n",
    "    if not process_dicts(name):\n",
    "        print(f\"File for {name} (all dicts) not found.\")"
   ],
   "id": "a5546b9fd9d9ce6e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from funcs.embeddings import node_characteristics",
   "id": "7a45cd9725ae419d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:11:52.048116Z",
     "start_time": "2024-12-24T23:11:52.045738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filename = 'data/kmers.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    random_kmers = pickle.load(f)\n",
    "    \n",
    "families_to_filter = ['LTR', 'Helitron', 'DNA/MuDR', 'LINE']\n",
    "families_dict = {type_str: idx for idx, type_str in enumerate(families_to_filter)}"
   ],
   "id": "65efac68b7daf55c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:18:29.628511Z",
     "start_time": "2024-12-24T23:18:29.602291Z"
    }
   },
   "cell_type": "code",
   "source": "families_dict",
   "id": "881fdbeaa12a7692",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LTR': 0, 'Helitron': 1, 'DNA/MuDR': 2, 'LINE': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T23:22:07.225142Z",
     "start_time": "2024-12-24T23:21:54.106647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = {}\n",
    "\n",
    "for name in names:\n",
    "    components = list(small_components_all[name]) + list(largest_component_all[name])\n",
    "    families = list(small_components_families_all[name]) + list(largest_component_families_all[name])\n",
    "\n",
    "    embeddings = []\n",
    "    \n",
    "    for component_nodes in components:\n",
    "        G_sub = G_all[name].subgraph(component_nodes)\n",
    "        \n",
    "        node_embeddings = []\n",
    "        for node in component_nodes:\n",
    "            embedding = node_characteristics(node, G_sub, node_freq_dict_all[name], random_kmers, node2name_dict_all[name], name2seq_all[name])\n",
    "            node_embeddings.append(embedding)\n",
    "        \n",
    "        embeddings.append(node_embeddings)\n",
    "    G = G_all[name]\n",
    "    \n",
    "    \n",
    "    train_data = []\n",
    "\n",
    "    for i in range(len(components)):\n",
    "        component_nodes = components[i]\n",
    "        G_sub = G.subgraph(component_nodes)\n",
    "    \n",
    "        # G_sub_new = G_new.subgraph([vertex_map[x] for x in component_nodes])\n",
    "    \n",
    "        target = [0] * len(families_to_filter)\n",
    "    \n",
    "        graph_embedding = nx_to_pyg_data(G_sub, embeddings[i], target)\n",
    "        train_data.append(graph_embedding)\n",
    "    \n",
    "\n",
    "    loader = DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "    \n",
    "    # Загрузка модели\n",
    "    model_filename = 'models_files/gnn.pkl'\n",
    "    with open(model_filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    print(\"Модель успешно загружена.\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            new_data = batch\n",
    "            output = model(new_data.x, new_data.edge_index, new_data.batch)\n",
    "    \n",
    "            prediction = output.argmax(dim=1)\n",
    "            prediction_int = prediction.item()\n",
    "            if name not in predictions:\n",
    "                predictions[name] = []\n",
    "            predictions[name].append(prediction_int)\n",
    "            \n",
    "            \n",
    "            if prediction_int == 3:\n",
    "                print(components[i])\n",
    "                break"
   ],
   "id": "43d4ad1eb057ac0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно загружена.\n",
      "{'R30342', 'R14592', 'N4246', 'R14587', 'R37769', 'R26845', 'N4058', 'R14589', 'R31237', 'R28350', 'N3552', 'R14586', 'R250', 'R29677', 'R14585', 'R14590', 'N3627', 'N5997', 'N2253', 'R14588', 'R16869', 'R37768', 'R14591', 'R4187'}\n",
      "Модель успешно загружена.\n",
      "{'R1059', 'N21', 'R293'}\n",
      "Модель успешно загружена.\n",
      "{'R1493', 'N277', 'R1769', 'R1767', 'N1141', 'N18', 'R546', 'R1768'}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for node in {'R30342', 'R14592', 'N4246', 'R14587', 'R37769', 'R26845', 'N4058', 'R14589', 'R31237', 'R28350', 'N3552', 'R14586', 'R250', 'R29677', 'R14585', 'R14590', 'N3627', 'N5997', 'N2253', 'R14588', 'R16869', 'R37768', 'R14591', 'R4187'}:\n",
    "    node_name = node2name_dict_all['juncea'][node]\n",
    "    seq = name2seq_all['juncea'][node_name]\n",
    "    if len(seq) >= 5000 and len(seq) < 6000:\n",
    "        print('>seq')\n",
    "        print(seq)\n",
    "    "
   ],
   "id": "acfad2dbced1619c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
