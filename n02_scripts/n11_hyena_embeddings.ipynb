{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ВАЖНО: перед запуском проверить флаг `generate_hyena_embeddings`",
   "id": "4010290243a16a46"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-09T10:07:59.120966Z",
     "start_time": "2025-09-09T10:07:55.061540Z"
    }
   },
   "source": [
    "import config\n",
    "\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "from hyena_dna.standalone_hyenadna import HyenaDNAModel, CharacterTokenizer\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "warnings.filterwarnings('ignore')\n",
    "os.chdir(config.DIR_ROOT)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nad/miniconda3/envs/mobiraph1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:07:59.199317Z",
     "start_time": "2025-09-09T10:07:59.196213Z"
    }
   },
   "cell_type": "code",
   "source": "from n02_scripts.n12_seq_from_fasta import seq_from_fasta",
   "id": "5c4f86034f7bcbc5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:07:59.383086Z",
     "start_time": "2025-09-09T10:07:59.218078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "\n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model"
   ],
   "id": "5a39e304c5ae9c2f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:08:03.502016Z",
     "start_time": "2025-09-09T10:07:59.390314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# instantiate pretrained model\n",
    "pretrained_model_name = 'hyenadna-small-32k-seqlen'\n",
    "max_length = 32_000\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    'checkpoints',\n",
    "    pretrained_model_name,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters\n",
    "    model_max_length=max_length,\n",
    ")"
   ],
   "id": "2a0a8c2bb582d459",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hyenadna-small-32k-seqlen'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Создание эмбеддингов для большого набора данных",
   "id": "3a1988d5ccc01f40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:08:41.284859Z",
     "start_time": "2025-09-09T10:08:41.282877Z"
    }
   },
   "cell_type": "code",
   "source": "generate_hyena_embeddings = True",
   "id": "96f96337699834a3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:28:18.953381Z",
     "start_time": "2025-09-09T10:28:18.189965Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if generate_hyena_embeddings:\n",
    "    # Подготовка\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    name_to_embedding = {}\n",
    "    name_to_type = {}\n",
    "\n",
    "    path_to_df_info = os.path.join(config.DIR_INCEST_MANY, 'repbase_orf_type.txt')\n",
    "    df_info = pd.read_csv(path_to_df_info, sep='\\t')\n",
    "    df_info = df_info[df_info['Good'] == 1]\n",
    "    fasta_path = os.path.join(config.DIR_INCEST_MANY, 'repbase.fasta')\n",
    "\n",
    "    # name_to_sequence = seq_from_fasta(fasta_path, list(df_info['name']))\n",
    "\n",
    "    row = df_info.iloc[0]\n",
    "    name = row['name']\n",
    "    type = row[\"MainType\"]\n",
    "    sequence = name_to_sequence[name].upper()\n",
    "\n",
    "    tokenized1 = tokenizer(sequence)[\"input_ids\"]\n",
    "    tok_tensor1 = torch.LongTensor(tokenized1).unsqueeze(0).to(\"cpu\")\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(tok_tensor1)  # (1, seq_len, hidden_dim)\n",
    "        with open(\"hyena_model_output.pkl\", \"wb\") as f:\n",
    "            pickle.dump(outputs, f)\n",
    "        embedding1 = outputs.mean(dim=1).squeeze(0).cpu().numpy()  # среднее по токенам\n",
    "\n",
    "    tokenized2 = tokenizer(sequence)[\"input_ids\"]\n",
    "    tok_tensor2 = torch.LongTensor(tokenized2).unsqueeze(0).to(\"cpu\")\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(tok_tensor2)  # (1, seq_len, hidden_dim)\n",
    "        embedding2 = outputs.mean(dim=1).squeeze(0).cpu().numpy()  # среднее по токенам\n",
    "\n",
    "    print(np.unique(tokenized2))\n",
    "    print(np.all(tokenized1 == tokenized2))\n",
    "    print(np.all(embedding1 == embedding2))\n",
    "\n",
    "\n",
    "\n",
    "    # for _, row in tqdm(df_info.iterrows(), total=len(df_info)):\n",
    "    #     name = row['name']\n",
    "    #     type = row[\"MainType\"]\n",
    "    #     sequence = name_to_sequence[name]\n",
    "    #\n",
    "    #     # Токенизация и преобразование\n",
    "    #     tokenized = tokenizer(sequence)[\"input_ids\"]\n",
    "    #     tok_tensor = torch.LongTensor(tokenized).unsqueeze(0).to(\"cpu\")\n",
    "    #\n",
    "    #     with torch.inference_mode():\n",
    "    #         outputs = model(tok_tensor)  # (1, seq_len, hidden_dim)\n",
    "    #         embedding = outputs.mean(dim=1).squeeze(0).cpu().numpy()  # среднее по токенам\n",
    "    #\n",
    "    #     name_to_embedding[name] = embedding\n",
    "    #     name_to_type[name] = type\n",
    "\n",
    "    # path_to_hyena_embedding = os.path.join(config.DIR_INCEST_MANY, 'hyena_embeddings_and_types.pkl')\n",
    "    # with open(path_to_hyena_embedding, \"wb\") as f:\n",
    "    #     pickle.dump({\"embeddings\": name_to_embedding, \"types\": name_to_type}, f)"
   ],
   "id": "e5f55b68491424fb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4371, 256])\n",
      "[ 0  1  7  8  9 10]\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:25:35.672748Z",
     "start_time": "2025-09-09T10:25:35.666960Z"
    }
   },
   "cell_type": "code",
   "source": "np.unique(tokenized1)",
   "id": "ff3f1484112f310b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:25:32.554039Z",
     "start_time": "2025-09-09T10:25:32.543223Z"
    }
   },
   "cell_type": "code",
   "source": "np.unique(list(sequence))",
   "id": "525e63dc621ec5df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'c', 'g', 't'], dtype='<U1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-09T10:08:03.661780Z",
     "start_time": "2025-09-09T10:08:03.556593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_to_hyena_embedding = os.path.join(config.DIR_INCEST_MANY, 'hyena_embeddings_and_types.pkl')\n",
    "with open(path_to_hyena_embedding, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "name_to_embedding = data[\"embeddings\"]\n",
    "name_to_type = data[\"types\"]"
   ],
   "id": "10e76b8a5e842081",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
