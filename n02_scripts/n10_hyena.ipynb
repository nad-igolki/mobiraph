{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ВАЖНО: перед запуском проверить флаг `generate_hyena_embeddings`",
   "id": "4010290243a16a46"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-04T10:22:36.238776Z",
     "start_time": "2025-09-04T10:22:31.974241Z"
    }
   },
   "source": [
    "import config\n",
    "\n",
    "import os\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import warnings\n",
    "import os\n",
    "from hyena_dna.standalone_hyenadna import HyenaDNAModel\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "warnings.filterwarnings('ignore')\n",
    "os.chdir(config.DIR_ROOT)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nad/miniconda3/envs/mobiraph1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:22:36.324176Z",
     "start_time": "2025-09-04T10:22:36.321578Z"
    }
   },
   "cell_type": "code",
   "source": "from n02_scripts.n11_seq_from_fasta import seq_from_fasta",
   "id": "5c4f86034f7bcbc5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:22:36.339432Z",
     "start_time": "2025-09-04T10:22:36.331656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ],
   "id": "6284afcbd309b53b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:22:36.420871Z",
     "start_time": "2025-09-04T10:22:36.345558Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "\n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model"
   ],
   "id": "5a39e304c5ae9c2f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:22:39.468867Z",
     "start_time": "2025-09-04T10:22:36.426462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# instantiate pretrained model\n",
    "pretrained_model_name = 'hyenadna-small-32k-seqlen'\n",
    "max_length = 32_000\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    'checkpoints',\n",
    "    pretrained_model_name,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],  # add DNA characters\n",
    "    model_max_length=max_length,\n",
    ")"
   ],
   "id": "2a0a8c2bb582d459",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Git hooks.\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hyenadna-small-32k-seqlen'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Создание эмбеддингов для большого набора данных",
   "id": "3a1988d5ccc01f40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T10:22:39.473885Z",
     "start_time": "2025-09-04T10:22:39.472296Z"
    }
   },
   "cell_type": "code",
   "source": "generate_hyena_embeddings = False",
   "id": "96f96337699834a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T12:19:18.173938Z",
     "start_time": "2025-09-04T10:22:39.486771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if generate_hyena_embeddings:\n",
    "    # Подготовка\n",
    "    model.to(\"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    name_to_embedding = {}\n",
    "    name_to_type = {}\n",
    "\n",
    "    path_to_df_info = os.path.join(config.DIR_INCEST_MANY, 'repbase_orf_type.txt')\n",
    "    df_info = pd.read_csv(path_to_df_info, sep='\\t')\n",
    "    df_info = df_info[df_info['Good'] == 1]\n",
    "    fasta_path = os.path.join(config.DIR_INCEST_MANY, 'repbase.fasta')\n",
    "\n",
    "    name_to_sequence = seq_from_fasta(fasta_path, list(df_info['name']))\n",
    "\n",
    "    for _, row in tqdm(df_info.iterrows(), total=len(df_info)):\n",
    "        name = row['name']\n",
    "        type = row[\"MainType\"]\n",
    "        sequence = name_to_sequence[name]\n",
    "\n",
    "        # Токенизация и преобразование\n",
    "        tokenized = tokenizer(sequence)[\"input_ids\"]\n",
    "        tok_tensor = torch.LongTensor(tokenized).unsqueeze(0).to(\"cpu\")\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(tok_tensor)  # (1, seq_len, hidden_dim)\n",
    "            embedding = outputs.mean(dim=1).squeeze(0).cpu().numpy()  # среднее по токенам\n",
    "\n",
    "        name_to_embedding[name] = embedding\n",
    "        name_to_type[name] = type\n",
    "\n",
    "    path_to_hyena_embedding = os.path.join(config.DIR_INCEST_MANY, 'hyena_embeddings_and_types.pkl')\n",
    "    with open(path_to_hyena_embedding, \"wb\") as f:\n",
    "        pickle.dump({\"embeddings\": name_to_embedding, \"types\": name_to_type}, f)"
   ],
   "id": "e5f55b68491424fb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sequences from repbase.fasta: 100%|██████████| 57732/57732 [01:49<00:00, 527.77it/s] \n",
      " 29%|██▉       | 16812/57732 [1:54:45<4:39:18,  2.44it/s] \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-04T12:19:21.715322Z",
     "start_time": "2025-09-04T12:19:21.479348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path_to_hyena_embedding = os.path.join(config.DIR_INCEST_MANY, 'hyena_embeddings_and_types.pkl')\n",
    "with open(path_to_hyena_embedding, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "name_to_embedding = data[\"embeddings\"]\n",
    "name_to_type = data[\"types\"]"
   ],
   "id": "10e76b8a5e842081",
   "outputs": [],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
